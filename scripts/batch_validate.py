#!/usr/bin/env python3
"""
æ‰¹é‡éªŒè¯æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†

è‡ªåŠ¨æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šã€‚

ç”¨æ³•:
    python batch_validate.py [--parallel] [--workers N]

å‚æ•°:
    --parallel: å¯ç”¨å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    --sequential: ä½¿ç”¨é¡ºåºæ‰§è¡Œæ¨¡å¼
    --workers N: å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°ï¼‰

è¾“å‡º:
    - results/batch_validation.json: æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç»“æœ
    - results/<model_name>_<device_name>/: å„ä¸ªæ¨¡å‹çš„å…·ä½“ç»“æœ
"""

import argparse
import json
import subprocess
import sys
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List

# é…ç½®
RESULTS_PATH = Path("results")
DATASETS_PATH = [
    Path("~/Resources/VaildVersion2/ABR-AL60").expanduser(),
    Path("~/Resources/VaildVersion2/Redmi-K30-Pro").expanduser(),
]
MODELS_PATH = Path("models")

# å¯ç”¨æ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = [
    "model_lstm_0105",
    "model_imot_0106",
    "model_imot_0111_64",
    "model_resnet_0111_96",
]


def run_validation(
    model: str, dataset_path: Path, verbose: bool = True
) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªæ¨¡å‹åœ¨å•ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯"""
    result = {
        "model": model,
        "dataset": str(dataset_path),
        "success": False,
        "error": None,
        "output": None,
        "result_path": None,
    }

    if verbose:
        print(f"\n{'=' * 50}")
        print(f"æµ‹è¯•æ¨¡å‹: {model} | æ•°æ®é›†: {dataset_path.name}")
        print(f"{'=' * 50}")

    cmd = [
        sys.executable,
        "VaildModel.py",
        "-d",
        str(dataset_path),
        "-m",
        model,
        "--models_path",
        str(MODELS_PATH),
    ]

    try:
        process = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
        )

        result["output"] = process.stdout
        result["error"] = process.stderr
        result["success"] = process.returncode == 0

        if result["success"]:
            result["result_path"] = str(RESULTS_PATH / f"{model}_{dataset_path.name}")
            if verbose:
                print(f"âœ… æˆåŠŸ: {model} on {dataset_path.name}")
        else:
            if verbose:
                print(f"âŒ å¤±è´¥: {model} on {dataset_path.name}")
                if process.stderr:
                    print(f"é”™è¯¯ä¿¡æ¯: {process.stderr[:200]}...")

    except subprocess.TimeoutExpired:
        result["error"] = "Timeout after 300 seconds"
        if verbose:
            print(f"â° è¶…æ—¶: {model} on {dataset_path.name}")
    except Exception as e:
        result["error"] = str(e)
        if verbose:
            print(f"ğŸ’¥ å¼‚å¸¸: {model} on {dataset_path.name} - {e}")

    return result


def generate_summary_report(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    summary = {
        "total_tests": len(results),
        "successful_tests": sum(1 for r in results if r["success"]),
        "failed_tests": sum(1 for r in results if not r["success"]),
        "models": {},
        "datasets": {},
        "results": results,
    }

    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    for model in AVAILABLE_MODELS:
        model_results = [r for r in results if r["model"] == model]
        summary["models"][model] = {
            "total": len(model_results),
            "successful": sum(1 for r in model_results if r["success"]),
            "failed": sum(1 for r in model_results if not r["success"]),
        }

    # æŒ‰æ•°æ®é›†ç»Ÿè®¡
    for dataset_path in DATASETS_PATH:
        dataset_results = [r for r in results if r["dataset"] == str(dataset_path)]
        summary["datasets"][dataset_path.name] = {
            "total": len(dataset_results),
            "successful": sum(1 for r in dataset_results if r["success"]),
            "failed": sum(1 for r in dataset_results if not r["success"]),
        }

    return summary


def run_validations_parallel(
    models: List[str], datasets: List[Path], max_workers: int | None = None
) -> List[Dict[str, Any]]:
    """å¹¶è¡Œè¿è¡Œæ‰€æœ‰éªŒè¯ä»»åŠ¡"""
    results = []
    total_tests = len(models) * len(datasets)
    completed_tests = 0

    # ç”¨äºçº¿ç¨‹å®‰å…¨çš„æ‰“å°
    print_lock = threading.Lock()

    def update_progress(result: Dict[str, Any]):
        nonlocal completed_tests
        with print_lock:
            completed_tests += 1
            status = "âœ…" if result["success"] else "âŒ"
            print(
                f"[{completed_tests}/{total_tests}] {status} {result['model']} on {Path(result['dataset']).name}"
            )

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [(model, dataset) for model in models for dataset in datasets]

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(run_validation, model, dataset, verbose=False): (
                model,
                dataset,
            )
            for model, dataset in tasks
        }

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_task):
            try:
                result = future.result()
                results.append(result)
                update_progress(result)
            except Exception as e:
                model, dataset = future_to_task[future]
                error_result = {
                    "model": model,
                    "dataset": str(dataset),
                    "success": False,
                    "error": f"Execution error: {str(e)}",
                    "output": None,
                    "result_path": None,
                }
                results.append(error_result)
                update_progress(error_result)

    return results


def run_validations_sequential(
    models: List[str], datasets: List[Path]
) -> List[Dict[str, Any]]:
    """é¡ºåºè¿è¡Œæ‰€æœ‰éªŒè¯ä»»åŠ¡"""
    results = []
    total_tests = len(models) * len(datasets)
    current_test = 0

    for model in models:
        for dataset_path in datasets:
            current_test += 1
            print(f"\nğŸ”„ è¿›åº¦: {current_test}/{total_tests}")
            result = run_validation(model, dataset_path, verbose=True)
            results.append(result)

    return results


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡éªŒè¯æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--parallel",
        "-p",
        action="store_true",
        default=True,
        help="å¯ç”¨å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰",
    )
    parser.add_argument(
        "--sequential", "-s", action="store_true", help="ä½¿ç”¨é¡ºåºæ‰§è¡Œæ¨¡å¼"
    )
    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        default=None,
        help="å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°ï¼‰",
    )
    return parser.parse_args()


def main():
    args = parse_args()

    # å¦‚æœæ˜ç¡®æŒ‡å®šäº† sequentialï¼Œåˆ™ç¦ç”¨å¹¶è¡Œ
    use_parallel = args.parallel and not args.sequential

    print("ğŸš€ å¼€å§‹æ‰¹é‡éªŒè¯æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†")
    print(f"æ¨¡å‹è·¯å¾„: {MODELS_PATH}")
    print(f"æ•°æ®é›†è·¯å¾„: {DATASETS_PATH}")

    # æ£€æŸ¥ç¯å¢ƒ
    if not MODELS_PATH.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODELS_PATH}")
        return

    # æ£€æŸ¥æ•°æ®é›†
    valid_datasets = []
    for dataset_path in DATASETS_PATH:
        if dataset_path.exists():
            valid_datasets.append(dataset_path)
        else:
            print(f"âš ï¸  æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")

    if not valid_datasets:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨æ•°æ®é›†")
        return

    print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {len(AVAILABLE_MODELS)} ä¸ª")
    for model in AVAILABLE_MODELS:
        model_file = MODELS_PATH / f"{model}.pt"
        status = "âœ…" if model_file.exists() else "âŒ"
        print(f"  {status} {model}")

    print(f"ğŸ“Š å¯ç”¨æ•°æ®é›†: {len(valid_datasets)} ä¸ª")
    for dataset_path in valid_datasets:
        print(f"  ğŸ“ {dataset_path.name}")

    # è‡ªåŠ¨ç»§ç»­æ‰§è¡Œ
    print("\nğŸ”„ å¼€å§‹æ‰§è¡Œæ‰€æœ‰æµ‹è¯•...")

    if use_parallel:
        print(
            f"âš¡ï¸ ä½¿ç”¨å¹¶è¡Œæ¨¡å¼æ‰§è¡Œæµ‹è¯•ï¼ˆæœ€å¤š {args.workers or 'CPUæ ¸å¿ƒæ•°'} ä¸ªå¹¶è¡Œä»»åŠ¡ï¼‰\n"
        )
        all_results = run_validations_parallel(
            models=AVAILABLE_MODELS, datasets=valid_datasets, max_workers=args.workers
        )
    else:
        print("ğŸ“ ä½¿ç”¨é¡ºåºæ¨¡å¼æ‰§è¡Œæµ‹è¯•\n")
        all_results = run_validations_sequential(
            models=AVAILABLE_MODELS, datasets=valid_datasets
        )

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\nğŸ“ˆ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    summary = generate_summary_report(all_results)

    # ä¿å­˜ç»“æœ
    RESULTS_PATH.mkdir(parents=True, exist_ok=True)
    report_path = RESULTS_PATH / "batch_validation.json"

    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    # æ‰“å°æ±‡æ€»
    print(f"\n{'=' * 60}")
    print("ğŸ‰ æ‰¹é‡éªŒè¯å®Œæˆ!")
    print(f"{'=' * 60}")
    print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"æˆåŠŸ: {summary['successful_tests']} âœ…")
    print(f"å¤±è´¥: {summary['failed_tests']} âŒ")
    print(f"æˆåŠŸç‡: {summary['successful_tests'] / summary['total_tests'] * 100:.1f}%")
    print(f"è¯¦ç»†æŠ¥å‘Š: {report_path}")

    # æŒ‰æ¨¡å‹æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š æŒ‰æ¨¡å‹ç»Ÿè®¡:")
    for model, stats in summary["models"].items():
        success_rate = (
            stats["successful"] / stats["total"] * 100 if stats["total"] > 0 else 0
        )
        print(
            f"  {model}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)"
        )

    # æŒ‰æ•°æ®é›†æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š æŒ‰æ•°æ®é›†ç»Ÿè®¡:")
    for dataset, stats in summary["datasets"].items():
        success_rate = (
            stats["successful"] / stats["total"] * 100 if stats["total"] > 0 else 0
        )
        print(
            f"  {dataset}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)"
        )


if __name__ == "__main__":
    main()
